{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "ROOT = \"\"\n",
    "DATA_PATH = \"data/\"\n",
    "OUTPUT_PATH = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69c4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ab6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(sentence):\n",
    "    incorrect_symbols = ['€', '≈', '¬', 'ќ', 'Ќ', '√', 'Ћ', '∆', 'ƒ', 'Є', '¤', 'ι', 'Ђ', 'Ў', 'ї', 'ћ', '▒', 'ﺎ', '\\xad', 'æ']\n",
    "    for symbol in incorrect_symbols:\n",
    "        if symbol in sentence:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d2f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"check\"] = train_df[\"correct_text\"].apply(check)\n",
    "train_df = train_df[train_df[\"check\"] == True]\n",
    "train_df = train_df.drop(columns=[\"check\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75c0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df[\"correct_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6792a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = {'!', '.', ',', '?', '/', '-', ';', '\"', \"'\", ':', '..', '...', '--', '---', '\"\"', \"''\", '…'}\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = list(filter(lambda word: word not in punct, sentence))\n",
    "    sentence = [\"<s>\"] + sentence + [\"</s>\"]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04f7bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'Считает', 'что', 'сможет', 'жить', 'вечно', '</s>'],\n",
       " ['<s>', '20', 'миллионов', 'и', 'ни', 'пенни', 'меньше', '</s>'],\n",
       " ['<s>', 'Но', 'и', 'мы', 'умрём', '</s>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = list(map(tokenize_sentence, train_text))\n",
    "tokenized_text[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d363b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "d = dict()\n",
    "for sentence in tokenized_text:\n",
    "    for first, second in list(ngrams(sentence, n=2)):\n",
    "        if first in d:\n",
    "            d[first][second] += 1\n",
    "        else:\n",
    "            d[first] = Counter()\n",
    "            d[first][second] += 1\n",
    "for key in d.keys():\n",
    "    d[key] = d[key].most_common(len(d[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cdeef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = dict()\n",
    "for sentence in tokenized_text:\n",
    "    for first, second in list(ngrams(sentence, n=2)):\n",
    "        if second in dr:\n",
    "            dr[second][first] += 1\n",
    "        else:\n",
    "            dr[second] = Counter()\n",
    "            dr[second][first] += 1\n",
    "for key in dr.keys():\n",
    "    dr[key] = dr[key].most_common(len(dr[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fc1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = dict()\n",
    "for sentence in tokenized_text:\n",
    "    for first, second, third in list(ngrams(sentence, n=3)):\n",
    "        if (first, second) in d3:\n",
    "            d3[(first, second)][third] += 1\n",
    "        else:\n",
    "            d3[(first, second)] = Counter()\n",
    "            d3[(first, second)][third] += 1\n",
    "for key in d3.keys():\n",
    "    d3[key] = d3[key].most_common(len(d3[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4f6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr3 = dict()\n",
    "for sentence in tokenized_text:\n",
    "    for first, second, third in list(ngrams(sentence, n=3)):\n",
    "        if (second, third) in dr3:\n",
    "            dr3[(second, third)][first] += 1\n",
    "        else:\n",
    "            dr3[(second, third)] = Counter()\n",
    "            dr3[(second, third)][first] += 1\n",
    "for key in dr3.keys():\n",
    "    dr3[key] = dr3[key].most_common(len(dr3[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab6b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = dict()\n",
    "for sentence in tokenized_text:\n",
    "    for first, second, third in list(ngrams(sentence, n=3)):\n",
    "        if (first, third) in dw:\n",
    "            dw[(first, third)][second] += 1\n",
    "        else:\n",
    "            dw[(first, third)] = Counter()\n",
    "            dw[(first, third)][second] += 1\n",
    "for key in dw.keys():\n",
    "    dw[key] = dw[key].most_common(len(dw[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f10b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_df[\"corrupted_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96cb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(DATA_PATH + \"private_test.csv\")\n",
    "test_X = test_df[\"corrupted_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd8b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def untokenize(words):\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387a88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "count = 0\n",
    "submit_df = pd.DataFrame(columns=[\"corrupted_text\"])\n",
    "\n",
    "def predict(eval_X):\n",
    "    global count, submit_df\n",
    "    for indx, X in enumerate(tqdm(eval_X)):\n",
    "        y_pred = [\"<s>\"]\n",
    "        tokenized = tokenize_sentence(X)\n",
    "        X_str = X\n",
    "        X = X.split()\n",
    "        for i in range(1, len(tokenized) - 1):\n",
    "            if tokenized[i] not in d.keys():\n",
    "                min_dist = 1e9\n",
    "                predict = tokenized[i]\n",
    "                if (y_pred[-1], tokenized[i + 1]) in dw.keys():\n",
    "                    for value, number in dw[(y_pred[-1], tokenized[i + 1])]:\n",
    "                        distance_ = distance(tokenized[i], value)\n",
    "                        if distance_ < min_dist and value != \"</s>\" and value != \"<s>\":\n",
    "                            min_dist = distance_\n",
    "                            predict = value\n",
    "                if i > 1:\n",
    "                    if (y_pred[-2], y_pred[-1]) in d3.keys():\n",
    "                        for value, number in d3[(y_pred[-2], y_pred[-1])]:\n",
    "                            distance_ = distance(tokenized[i], value)\n",
    "                            if distance_ < min_dist and value != \"</s>\" and value != \"<s>\":\n",
    "                                min_dist = distance_\n",
    "                                predict = value\n",
    "                if y_pred[-1] in d.keys():\n",
    "                    for value, number in d[y_pred[-1]]:\n",
    "                        distance_ = distance(tokenized[i], value)\n",
    "                        if distance_ < min_dist and value != \"</s>\" and value != \"<s>\":\n",
    "                            min_dist = distance_\n",
    "                            predict = value\n",
    "                if i < len(tokenized) - 2:\n",
    "                    if (tokenized[i + 1], tokenized[i + 2]) in dr3.keys():\n",
    "                        for value, number in dr3[(tokenized[i + 1], tokenized[i + 2])]:\n",
    "                            distance_ = distance(tokenized[i], value)\n",
    "                            if distance_ < min_dist and value != \"</s>\" and value != \"<s>\":\n",
    "                                min_dist = distance_\n",
    "                                predict = value\n",
    "                if tokenized[i + 1] in dr.keys():\n",
    "                    for value, number in dr[tokenized[i + 1]]:\n",
    "                        distance_ = distance(tokenized[i], value)\n",
    "                        if distance_ < min_dist and value != \"</s>\" and value != \"<s>\":\n",
    "                            min_dist = distance_\n",
    "                            predict = value\n",
    "                y_pred += [predict]\n",
    "            else:\n",
    "                y_pred += [tokenized[i]]\n",
    "        y_pred = y_pred[1:]\n",
    "        answer = list()\n",
    "        X_str_tokenized = word_tokenize(X_str)\n",
    "        j = 0\n",
    "        for i in range(len(X_str_tokenized)):\n",
    "            if not list(filter(lambda word: word not in punct, word_tokenize(X_str_tokenized[i]))) or X_str_tokenized[i] == ' ':\n",
    "                answer.append(X_str_tokenized[i])\n",
    "            else:\n",
    "                if X_str_tokenized[i][0] in punct:\n",
    "                    y_pred[j] = X_str_tokenized[i][0] + y_pred[j]\n",
    "                if (X_str_tokenized[i].istitle() or j == 0) and '-' not in y_pred[j]:\n",
    "                    answer.append(y_pred[j].title())\n",
    "                else:\n",
    "                    answer.append(y_pred[j])\n",
    "                j += 1\n",
    "        y_pred = untokenize(answer)\n",
    "        submit_df.loc[len(submit_df.index)] = [y_pred]\n",
    "#         if indx == 100:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73488fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483d0fd838374bc1a10f8cb0401fc874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397651ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = submit_df.rename(columns={\"corrupted_text\": submit_df[\"corrupted_text\"][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35b3ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.drop([0]).reset_index().drop(columns=['index']).to_csv(OUTPUT_PATH + \"private.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02725034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
